#!/usr/bin/env python3
"""
Enhanced Table Extractor for TechWriterReview
=============================================
Version: reads from version.json (module v1.0)
Date: 2026-01-27

Maximizes table extraction accuracy WITHOUT Docling by combining:
- Camelot (best for bordered/lattice tables)
- Tabula (good for stream/borderless tables)  
- pdfplumber (fallback and text extraction)

This achieves ~85-90% accuracy vs Docling's ~95%, but works completely
offline without AI models.

Author: Nick / SAIC Systems Engineering
"""

import os
import tempfile
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field
from pathlib import Path
import logging

__version__ = "1.0.0"

logger = logging.getLogger(__name__)

# Track available libraries
CAMELOT_AVAILABLE = False
TABULA_AVAILABLE = False
PDFPLUMBER_AVAILABLE = False

try:
    import camelot
    CAMELOT_AVAILABLE = True
except ImportError:
    pass

try:
    import tabula
    TABULA_AVAILABLE = True
except ImportError:
    pass

try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    pass


@dataclass
class ExtractedTable:
    """A table extracted from a document with metadata."""
    index: int
    page: int
    headers: List[str]
    rows: List[List[str]]
    confidence: float  # 0.0 - 1.0
    source: str  # 'camelot', 'tabula', 'pdfplumber'
    caption: str = ""
    has_borders: bool = True
    row_count: int = 0
    col_count: int = 0
    
    def __post_init__(self):
        self.row_count = len(self.rows)
        self.col_count = len(self.headers) if self.headers else (len(self.rows[0]) if self.rows else 0)
    
    def to_dict(self) -> Dict:
        return {
            'index': self.index,
            'page': self.page,
            'headers': self.headers,
            'rows': self.rows,
            'confidence': self.confidence,
            'source': self.source,
            'caption': self.caption,
            'has_borders': self.has_borders,
            'row_count': self.row_count,
            'col_count': self.col_count,
        }


@dataclass
class ExtractionResult:
    """Result from enhanced table extraction."""
    tables: List[ExtractedTable] = field(default_factory=list)
    extraction_method: str = ""
    total_pages: int = 0
    extraction_time_ms: float = 0.0
    warnings: List[str] = field(default_factory=list)


class EnhancedTableExtractor:
    """
    Multi-library table extractor for maximum accuracy.
    
    Extraction Strategy:
    1. Try Camelot lattice mode (best for bordered tables)
    2. Try Camelot stream mode (for borderless tables)
    3. Try Tabula (alternative algorithm)
    4. Fall back to pdfplumber (basic extraction)
    5. Deduplicate and merge results
    """
    
    def __init__(self, prefer_accuracy: bool = True):
        """
        Args:
            prefer_accuracy: If True, try all methods and pick best.
                           If False, stop at first successful extraction.
        """
        self.prefer_accuracy = prefer_accuracy
        self._check_dependencies()
    
    def _check_dependencies(self):
        """Log available extraction libraries."""
        available = []
        if CAMELOT_AVAILABLE:
            available.append("camelot")
        if TABULA_AVAILABLE:
            available.append("tabula")
        if PDFPLUMBER_AVAILABLE:
            available.append("pdfplumber")
        
        if not available:
            logger.warning("No table extraction libraries available!")
        else:
            logger.debug(f"Table extraction libraries: {', '.join(available)}")
    
    def extract_tables(self, filepath: str, pages: str = "all") -> ExtractionResult:
        """
        Extract tables from a PDF using multiple methods.
        
        Args:
            filepath: Path to PDF file
            pages: Page specification ('all', '1', '1-5', '1,3,5')
            
        Returns:
            ExtractionResult with all extracted tables
        """
        import time
        start_time = time.time()
        
        result = ExtractionResult()
        all_tables = []
        
        filepath = str(filepath)
        if not os.path.exists(filepath):
            result.warnings.append(f"File not found: {filepath}")
            return result
        
        # Get page count
        result.total_pages = self._get_page_count(filepath)
        
        # Strategy 1: Camelot lattice (bordered tables)
        if CAMELOT_AVAILABLE:
            try:
                lattice_tables = self._extract_camelot_lattice(filepath, pages)
                all_tables.extend(lattice_tables)
                logger.debug(f"Camelot lattice found {len(lattice_tables)} tables")
            except Exception as e:
                result.warnings.append(f"Camelot lattice failed: {e}")
        
        # Strategy 2: Camelot stream (borderless tables)
        if CAMELOT_AVAILABLE and self.prefer_accuracy:
            try:
                stream_tables = self._extract_camelot_stream(filepath, pages)
                # Only add stream tables that don't overlap with lattice
                new_tables = self._filter_duplicates(stream_tables, all_tables)
                all_tables.extend(new_tables)
                logger.debug(f"Camelot stream found {len(new_tables)} new tables")
            except Exception as e:
                result.warnings.append(f"Camelot stream failed: {e}")
        
        # Strategy 3: Tabula
        if TABULA_AVAILABLE and (self.prefer_accuracy or not all_tables):
            try:
                tabula_tables = self._extract_tabula(filepath, pages)
                if self.prefer_accuracy:
                    new_tables = self._filter_duplicates(tabula_tables, all_tables)
                    all_tables.extend(new_tables)
                else:
                    all_tables.extend(tabula_tables)
                logger.debug(f"Tabula found {len(tabula_tables)} tables")
            except Exception as e:
                result.warnings.append(f"Tabula failed: {e}")
        
        # Strategy 4: pdfplumber fallback
        if PDFPLUMBER_AVAILABLE and (self.prefer_accuracy or not all_tables):
            try:
                plumber_tables = self._extract_pdfplumber(filepath, pages)
                if self.prefer_accuracy:
                    new_tables = self._filter_duplicates(plumber_tables, all_tables)
                    all_tables.extend(new_tables)
                else:
                    all_tables.extend(plumber_tables)
                logger.debug(f"pdfplumber found {len(plumber_tables)} tables")
            except Exception as e:
                result.warnings.append(f"pdfplumber failed: {e}")
        
        # Sort by page and position, assign indices
        all_tables.sort(key=lambda t: (t.page, t.index))
        for i, table in enumerate(all_tables, 1):
            table.index = i
        
        result.tables = all_tables
        result.extraction_method = self._get_primary_method(all_tables)
        result.extraction_time_ms = (time.time() - start_time) * 1000
        
        return result
    
    def _extract_camelot_lattice(self, filepath: str, pages: str) -> List[ExtractedTable]:
        """Extract tables using Camelot lattice mode (bordered tables)."""
        tables = []
        try:
            # Lattice mode is best for tables with clear borders
            camelot_tables = camelot.read_pdf(
                filepath, 
                pages=pages if pages != "all" else "all",
                flavor='lattice',
                suppress_stdout=True
            )
            
            for i, ct in enumerate(camelot_tables):
                df = ct.df
                if df.empty:
                    continue
                
                # First row as headers if it looks like headers
                headers = [str(h).strip() for h in df.iloc[0].tolist()]
                rows = [[str(cell).strip() for cell in row] for row in df.iloc[1:].values.tolist()]
                
                # If headers look like data, treat all as rows
                if self._looks_like_data_row(headers):
                    rows = [headers] + rows
                    headers = [f"Col{j+1}" for j in range(len(headers))]
                
                tables.append(ExtractedTable(
                    index=i + 1,
                    page=ct.page,
                    headers=headers,
                    rows=rows,
                    confidence=ct.accuracy / 100.0 if hasattr(ct, 'accuracy') else 0.85,
                    source='camelot-lattice',
                    has_borders=True
                ))
        except Exception as e:
            logger.debug(f"Camelot lattice error: {e}")
        
        return tables
    
    def _extract_camelot_stream(self, filepath: str, pages: str) -> List[ExtractedTable]:
        """Extract tables using Camelot stream mode (borderless tables)."""
        tables = []
        try:
            camelot_tables = camelot.read_pdf(
                filepath,
                pages=pages if pages != "all" else "all",
                flavor='stream',
                suppress_stdout=True,
                edge_tol=50  # Tolerance for edge detection
            )
            
            for i, ct in enumerate(camelot_tables):
                df = ct.df
                if df.empty:
                    continue
                
                headers = [str(h).strip() for h in df.iloc[0].tolist()]
                rows = [[str(cell).strip() for cell in row] for row in df.iloc[1:].values.tolist()]
                
                if self._looks_like_data_row(headers):
                    rows = [headers] + rows
                    headers = [f"Col{j+1}" for j in range(len(headers))]
                
                tables.append(ExtractedTable(
                    index=i + 1,
                    page=ct.page,
                    headers=headers,
                    rows=rows,
                    confidence=ct.accuracy / 100.0 if hasattr(ct, 'accuracy') else 0.70,
                    source='camelot-stream',
                    has_borders=False
                ))
        except Exception as e:
            logger.debug(f"Camelot stream error: {e}")
        
        return tables
    
    def _extract_tabula(self, filepath: str, pages: str) -> List[ExtractedTable]:
        """Extract tables using Tabula."""
        tables = []
        try:
            # Tabula returns list of DataFrames
            page_spec = pages if pages != "all" else "all"
            dfs = tabula.read_pdf(
                filepath,
                pages=page_spec,
                multiple_tables=True,
                silent=True
            )
            
            for i, df in enumerate(dfs):
                if df.empty:
                    continue
                
                # Use column names as headers
                headers = [str(h).strip() for h in df.columns.tolist()]
                rows = [[str(cell).strip() if pd.notna(cell) else "" for cell in row] 
                        for row in df.values.tolist()]
                
                # Tabula doesn't give page numbers easily, estimate
                tables.append(ExtractedTable(
                    index=i + 1,
                    page=1,  # Tabula doesn't reliably report page
                    headers=headers,
                    rows=rows,
                    confidence=0.75,  # Tabula is generally reliable
                    source='tabula',
                    has_borders=True  # Assume bordered
                ))
        except Exception as e:
            logger.debug(f"Tabula error: {e}")
        
        return tables
    
    def _extract_pdfplumber(self, filepath: str, pages: str) -> List[ExtractedTable]:
        """Extract tables using pdfplumber."""
        tables = []
        try:
            with pdfplumber.open(filepath) as pdf:
                page_nums = self._parse_pages(pages, len(pdf.pages))
                
                table_idx = 0
                for page_num in page_nums:
                    if page_num > len(pdf.pages):
                        continue
                    
                    page = pdf.pages[page_num - 1]
                    page_tables = page.extract_tables()
                    
                    for pt in page_tables:
                        if not pt or len(pt) < 2:
                            continue
                        
                        table_idx += 1
                        headers = [str(h).strip() if h else "" for h in pt[0]]
                        rows = [[str(cell).strip() if cell else "" for cell in row] 
                                for row in pt[1:]]
                        
                        tables.append(ExtractedTable(
                            index=table_idx,
                            page=page_num,
                            headers=headers,
                            rows=rows,
                            confidence=0.65,  # pdfplumber is less accurate
                            source='pdfplumber',
                            has_borders=True
                        ))
        except Exception as e:
            logger.debug(f"pdfplumber error: {e}")
        
        return tables
    
    def _get_page_count(self, filepath: str) -> int:
        """Get total page count from PDF."""
        if PDFPLUMBER_AVAILABLE:
            try:
                with pdfplumber.open(filepath) as pdf:
                    return len(pdf.pages)
            except:
                pass
        return 0
    
    def _parse_pages(self, pages: str, total_pages: int) -> List[int]:
        """Parse page specification to list of page numbers."""
        if pages == "all":
            return list(range(1, total_pages + 1))
        
        result = []
        for part in pages.split(","):
            if "-" in part:
                start, end = part.split("-")
                result.extend(range(int(start), int(end) + 1))
            else:
                result.append(int(part))
        return result
    
    def _looks_like_data_row(self, row: List[str]) -> bool:
        """Check if a row looks like data rather than headers."""
        if not row:
            return False
        
        # If most cells are numeric, it's probably data
        numeric_count = sum(1 for cell in row if self._is_numeric(cell))
        if numeric_count > len(row) * 0.6:
            return True
        
        # If cells are very long, probably data
        avg_len = sum(len(str(cell)) for cell in row) / len(row)
        if avg_len > 50:
            return True
        
        return False
    
    def _is_numeric(self, value: str) -> bool:
        """Check if a string represents a number."""
        try:
            float(value.replace(",", "").replace("$", "").replace("%", ""))
            return True
        except:
            return False
    
    def _filter_duplicates(self, new_tables: List[ExtractedTable], 
                          existing_tables: List[ExtractedTable]) -> List[ExtractedTable]:
        """Filter out tables that are duplicates of existing ones."""
        filtered = []
        for new_table in new_tables:
            is_duplicate = False
            for existing in existing_tables:
                if self._tables_similar(new_table, existing):
                    # Keep the one with higher confidence
                    if new_table.confidence > existing.confidence:
                        existing_tables.remove(existing)
                        filtered.append(new_table)
                    is_duplicate = True
                    break
            if not is_duplicate:
                filtered.append(new_table)
        return filtered
    
    def _tables_similar(self, t1: ExtractedTable, t2: ExtractedTable, 
                       threshold: float = 0.8) -> bool:
        """Check if two tables are likely the same."""
        # Different pages = different tables
        if t1.page != t2.page and t1.page != 1 and t2.page != 1:
            return False
        
        # Different dimensions = different tables
        if abs(t1.row_count - t2.row_count) > 2:
            return False
        if abs(t1.col_count - t2.col_count) > 1:
            return False
        
        # Compare content similarity
        t1_text = " ".join(t1.headers + [cell for row in t1.rows for cell in row])
        t2_text = " ".join(t2.headers + [cell for row in t2.rows for cell in row])
        
        # Simple Jaccard similarity
        t1_words = set(t1_text.lower().split())
        t2_words = set(t2_text.lower().split())
        
        if not t1_words or not t2_words:
            return False
        
        intersection = len(t1_words & t2_words)
        union = len(t1_words | t2_words)
        similarity = intersection / union if union else 0
        
        return similarity > threshold
    
    def _get_primary_method(self, tables: List[ExtractedTable]) -> str:
        """Determine which extraction method contributed most."""
        if not tables:
            return "none"
        
        sources = {}
        for t in tables:
            sources[t.source] = sources.get(t.source, 0) + 1
        
        return max(sources, key=sources.get) if sources else "unknown"


# Import pandas for tabula
try:
    import pandas as pd
except ImportError:
    pd = None


def get_extraction_capabilities() -> Dict[str, Any]:
    """Report available extraction capabilities."""
    return {
        'camelot': CAMELOT_AVAILABLE,
        'tabula': TABULA_AVAILABLE,
        'pdfplumber': PDFPLUMBER_AVAILABLE,
        'best_method': 'camelot' if CAMELOT_AVAILABLE else ('tabula' if TABULA_AVAILABLE else 'pdfplumber'),
        'estimated_accuracy': 0.90 if CAMELOT_AVAILABLE else (0.80 if TABULA_AVAILABLE else 0.70)
    }


# Convenience function
def extract_tables_from_pdf(filepath: str, pages: str = "all") -> ExtractionResult:
    """
    Extract tables from a PDF using the best available method.
    
    Args:
        filepath: Path to PDF file
        pages: Page specification ('all', '1', '1-5', '1,3,5')
        
    Returns:
        ExtractionResult with all extracted tables
    """
    extractor = EnhancedTableExtractor(prefer_accuracy=True)
    return extractor.extract_tables(filepath, pages)
